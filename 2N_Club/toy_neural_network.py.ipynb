{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ce ne gândim când începem să conturăm arhitectura unei rețele?\n",
    "- **inițializare**: \n",
    "    - *numărul de noduri de input, output și ascunse*. Altfel spus, stabilim forma și dimensiunile rețelei în etapa aceasta. Vom alege să nu hard codăm acești parametri ci să permitem setarea lor pentru fiecare obiect NeuralNetwork în parte.\n",
    "    - *legături (weights)* - le folosim ca să propagăm înainte semnalul de intrare prin rețea, precum și ca să propagăm înapoi erorile și să actualizăm aportul adus de acele legături la predicția finală. (De revăzut slide-urile din prima parte a workshop-ului pentru o imagine mai clară).\n",
    "    Cum reprezentăm weight-urile? \n",
    "    Ca matrici: \n",
    "    *Wih* - matrice no_hidden x no_input, pentru legăturile dintre stratul de intrare și cel ascuns\n",
    "    *Who* - matrice no_output x no_hidden, pentru legăturile dintre stratul ascuns și cel de ieșire\n",
    "    \n",
    "- **antrenare**: rafinarea weight-urilor (parametrii rețelei)\n",
    "    - aici avem două părți: \n",
    "        1. Feed-forward: pasăm input-ul prin rețea până obținem un output.\n",
    "        2. Backpropagation: calculăm eroarea (diferența) dintre output-ul obținut și ce ne-am fi așteptat să obținem (ground truth) și pe baza acesteia actualizăm parametrii rețelei (weight-urile).\n",
    "- **testare**: \n",
    "    - folosim rețeaua antrenată pe date noi, pe care aceasta nu le-a mai văzut până acum.\n",
    "    - trebuie executată pasarea înainte (forward pass) a datelor:\n",
    "        1. input->hidden: X_hidden = W_ih * I: \n",
    "        ceea ce intră în stratul ascuns este de fapt input-ul moderat al rețelei, în funcție de cât de puternice sunt legăturile dintre neuronii de pe stratul de intrare și corespondenții de pe stratul ascuns.\n",
    "        2. hidden->output: O_hidden = sigmoid(X_hidden):\n",
    "        pur și simplu aplicăm funcția sigmoidală pe input-urile stratului ascuns.\n",
    "        3. 4. Similar cu 1. & 2. pentru stratul final (de ieșire)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, no_input, no_hidden, no_output, lr):\n",
    "        \"\"\"\n",
    "        Initialize the neural network: set the number of neurons in each layer and the learning rate.\n",
    "        \"\"\"\n",
    "        self.no_input = no_input\n",
    "        self.no_output = no_output\n",
    "        self.no_hidden = no_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Initialize the weights. They are also an important part of the network.\n",
    "        # Method 1:\n",
    "        # rand(r, c) gives a rxc marix filled with numbers randomly picked from the (0, 1) interval. \n",
    "        # As the weights can also be negative, we can subtract 0.5 and generate weights in the (-0.5, 0.5) interval.\n",
    "        self.W_ih =  np.random.rand(no_hidden, no_input) - 0.5\n",
    "        self.W_ho =  np.random.rand(no_output, no_hidden) - 0.5\n",
    "        \n",
    "        # Method 2:\n",
    "        # Sample from a normal distribution with mean = 0 (centered) and the standard deviation 1/sqrt(incoming links)\n",
    "        self.W_ih =  np.random.normal(0.0, pow(no_input, - 0.5), (no_hidden, no_input))\n",
    "        self.W_ho =  np.random.normal(0.0, pow(no_hidden, - 0.5), (no_output, no_hidden))\n",
    "        \n",
    "        # The activation function (sigmoid)\n",
    "        # Note: \n",
    "        # Here we have used a shorter way of defining a function. Instead of using def we have used lambda.\n",
    "        # Lambda gives us an anonymous function whose definition can appear anywhere in the code.\n",
    "        # We have assigned a name to this definition as we'll want to use it multiple times from now on.\n",
    "        # expit is the implementation of the sigmoid function defined in scipy.\n",
    "        self.activation_function = lambda x: ss.expit(x)\n",
    "        \n",
    "    def feed_forward(self, inputs):\n",
    "        \n",
    "        # Convert the list of inputs into a 2D array\n",
    "        I = np.array(inputs, ndmin=2).T\n",
    "        \n",
    "        # X_hidden = W_ih * I\n",
    "        X_hidden = np.dot(self.W_ih, I)\n",
    "        \n",
    "        # O_hidden = sigmoid(X_hidden): the output of the hidden layer is its input squashed using the sigmoid function\n",
    "        O_hidden = self.activation_function(X_hidden)\n",
    "        \n",
    "        # X_final = W_ho * O_hidden: the signals into the final output layer\n",
    "        X_final = np.dot(self.W_ho, O_hidden)\n",
    "        \n",
    "        # O_final = sigmoid(X_final)\n",
    "        O_final = self.activation_function(X_final)\n",
    "        \n",
    "        return O_final, O_hidden, I\n",
    "    \n",
    "    def train(self, inputs, targets):\n",
    "        # Feed Forward\n",
    "        O_final, O_hidden, I = self.feed_forward(inputs)\n",
    "        \n",
    "        # Backpropagation\n",
    "        # Convert the list of target values into a 2D array\n",
    "        T = np.array(targets, ndmin=2).T\n",
    "        \n",
    "        # Compute the error: E_output = T - O_final\n",
    "        E_output = T - O_final\n",
    "        \n",
    "        # Compute the backpropagated errors for the hidden layer nodes\n",
    "        # E_hidden = transposed(W_ho) * E_output\n",
    "        E_hidden = np.dot(self.W_ho.T, E_output)\n",
    "        \n",
    "        # Update the weights between the hidden and output layers using the formula obtained in the first part of the workshop\n",
    "        # delta_w_jk = lr * E_k *sigmoid(O_k) * (1 - sigmoid(O_k)) * trasposed(O_j)\n",
    "        self.W_ho += self.lr * np.dot((E_output * O_final * (1 - O_final)), np.transpose(O_hidden))\n",
    "        \n",
    "        # Update the weights between the hidden and the input layer\n",
    "        self.W_ih += self.lr * np.dot((E_hidden * O_hidden * (1 - O_hidden)), np.transpose(I))\n",
    "        \n",
    "    \n",
    "    def test(self, inputs):\n",
    "        \"\"\"\n",
    "        Pass the input of the network through the layers and return the corresponding output.\n",
    "        \"\"\"\n",
    "        O_final, _, _ = self.feed_forward(inputs)               \n",
    "        return O_final\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haideți să ne construim propria rețea - adică o instanță a clasei NeuralNetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53824619],\n",
       "       [0.42680568],\n",
       "       [0.50312207]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of neurons on each layer\n",
    "no_input = 3; no_output = 3; no_hidden = 3\n",
    "\n",
    "# learning rate\n",
    "lr = 0.3\n",
    "\n",
    "# create the neural network\n",
    "nn = NeuralNetwork(no_input, no_hidden, no_output, lr)\n",
    "\n",
    "# pick some random numbers to check just to be sure there are no errors\n",
    "nn.test([1.0, 0.5, -1.5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
