{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_neural_network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Qb60ApNrsIAt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recunoașterea Cifrelor Scrise de Mână\n",
        "\n",
        "### Setul de date MNIST\n",
        "\n",
        "MNIST este un set de date compus din diverse imagini cu cifrele de la 0 la 9 scrise de mână. Această aplicație a rețelelor neurale este una extrem de populară în mediul academic și orice tentativă (auto)didactică implicând rețelele neurale. Motivul pentru aceasta este dat de faptul că avem un singur canal de culoare pentru imaginile ce compun MNIST (aceste imagini sunt alb-negru). Această proprietate ne simplifică destul de mult munca. Setul de date are și un număr rezonabil de exemple, atât pentru antrenare cât și de test. Prin urmare putem să facem antrenarea chiar și pe CPU, chit că durează mai mult.\n",
        "\n",
        "Putem lua setul acesta de date de aici: http://yann.lecun.com/exdb/mnist.\n",
        "\n",
        "Noi vom lucra cu setul de date simplificat, ce se găsește aici:\n",
        "- training set:\n",
        "    - 60,000 de exemple etichetate (labelled) pe care le vom folosi ca să ne antrenăm rețeaua:\n",
        "    https://pjreddie.com/media/files/mnist_train.csv\n",
        "    \n",
        "- test set:\n",
        "    - 10,000 de samples în setul pe care-l folosim pentru efectuarea de teste asupra rețelei, pentru a vedea cum evoluează ea și cum se îmbunătățesc rezultatele pe date pe care rețeaua nu le-a văzut în faza de antrenare.\n",
        "    https://pjreddie.com/media/files/mnist_test.csv\n",
        "    \n",
        "Structura celor două csv-uri (comma separated values) este, după cum urmează:\n",
        "- label: cifre de la 0 la 9 ---> cifra ce se regăsește în imagine de fapt.\n",
        "- valorile pixelilor din imagine. Vectorul de pixeli este 2D și are dimensiunile 28x28. Deci în csv vom observa 784 de valori după etichetă.\n",
        "\n",
        "### 1. Prelucrarea Datelor\n",
        "    1.1. Deschidem fișierele csv ce conțin datele și citim conținutul acestora apoi le închidem.\n",
        "    1.2. Folosim virgula ca separator și obținem un vector de elemente din fiecare linie citită. Convertim vectorul de 784 valori de pixeli într-o matrice 28x28. Apoi afișăm imaginea pentru a ne face o idee vizuală despre setul de date.\n",
        "    1.3. Transformăm valorile pixelilor din intervalul [0, 255] în [0, 1]. \n",
        "         Cum facem asta?\n",
        "       - împărțirea la 255 a output-urilor raw din intervalul [0, 255]. \n",
        "         => Astfel le aducem în intervalul [0, 1]. \n",
        "    1.4. Pe prima poziție din csv avem clasa căreia aparține elementul corect (ce imagine afișează acesta). \n",
        "         Vrem să avem un vector de probabilități, mai degrabă decât un singur număr. Astfel alegem un vector de 10 elemente ca și target (cel cu care vom compara output-ul rețelei). Vom avea 1 pe poziția corespunzătoare cifrei din imagine (label-ului) și 0 pe restul pozițiilor."
      ]
    },
    {
      "metadata": {
        "id": "Eb2UGzqLsIAy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f0372c1e-5d99-4872-b905-25efa20c07ff"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plot\n",
        "import scipy.special\n",
        "import scipy.ndimage\n",
        "%matplotlib inline\n",
        "\n",
        "!ls mnist_train.csv || wget https://pjreddie.com/media/files/mnist_train.csv\n",
        "!ls mnist_test.csv || wget https://pjreddie.com/media/files/mnist_test.csv"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mnist_train.csv\n",
            "mnist_test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bgLiSqCDsIA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "95fddc38-7261-4d94-987b-6627d42a240b"
      },
      "cell_type": "code",
      "source": [
        "# 1.1. Read the data\n",
        "\n",
        "# i) Open the csv file containing the data\n",
        "train_data_file = open(\"mnist_train.csv\", \"r\")\n",
        "# ii) Read all the contents of the file as a list of lines (HINT: readlines)\n",
        "train_data_list = train_data_file.readlines()\n",
        "# iii) Close the file\n",
        "train_data_file.close()\n",
        "\n",
        "# How many samples?\n",
        "print(\"Total samples:\", len(train_data_list))\n",
        "\n",
        "# 1.2. Extract and reshape\n",
        "\n",
        "# i) What you've just read is lines (strings). Split by comma the values on line 10 to obtain a vector of pixels.\n",
        "all_values = train_data_list[10].split(',')\n",
        "\n",
        "# ii) We ignore the first value on the line, which is the label  => all_values[1:]\n",
        "#     Use asfarray to convert a string to a real number array. \n",
        "#     Do not forget that what we have read from the file is a string!\n",
        "#     Use reshape to make a matrix from the line => 28x28\n",
        "# HINT: np.asfarray, reshape\n",
        "image_array = np.asfarray(all_values[1:]).reshape((28, 28))\n",
        "\n",
        "# iii) Plot the image of this current handwritten digit\n",
        "# HINT: plot.imshow\n",
        "plot.imshow(image_array, cmap='Greys', interpolation=None)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total samples: 60000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f09d8977278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDLqUAYMuZcCgSxn4f3cZ72FWDfluAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f09d8963c18>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "t4OKQQ0msIBB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2244
        },
        "outputId": "9af060d7-8a95-4f99-dea2-e8e7f07c628f"
      },
      "cell_type": "code",
      "source": [
        "# 1.3. Process the data\n",
        "#      Scale the pixel values from [0, 255] to the range [0.0, 1.0].\n",
        "# HINT: all_values[1:], np.asfarray\n",
        "scaled_input = (np.asfarray(all_values[1:]) / 255.0)\n",
        "print(scaled_input)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.16470588 0.4627451  0.85882353\n",
            " 0.65098039 0.4627451  0.4627451  0.02352941 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.40392157 0.94901961 0.99607843 0.99607843 0.99607843 0.99607843\n",
            " 0.99607843 0.25882353 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.90980392\n",
            " 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843 0.93333333\n",
            " 0.2745098  0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.40784314 0.95686275 0.99607843\n",
            " 0.87843137 0.99607843 0.99607843 0.99607843 0.55294118 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.81176471 0.99607843 0.82352941 0.99607843\n",
            " 0.99607843 0.99607843 0.13333333 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.32941176 0.80784314 0.99607843 0.99607843 0.99607843 0.99607843\n",
            " 0.16078431 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.09411765\n",
            " 0.81960784 0.99607843 0.99607843 0.99607843 0.67058824 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.35686275 0.5372549  0.99215686 0.99607843\n",
            " 0.99607843 0.99607843 0.43921569 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15686275 0.83921569\n",
            " 0.98039216 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
            " 0.13333333 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.31764706 0.96862745 0.99607843 0.99607843\n",
            " 0.99607843 0.99607843 0.99607843 0.99607843 0.57254902 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.43137255 0.96470588 0.99607843 0.99607843 0.99607843\n",
            " 0.99607843 0.99607843 0.67058824 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.28627451 0.34901961 0.34901961 0.36470588 0.94117647 0.99607843\n",
            " 0.67058824 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00392157 0.50196078 0.99607843 0.85882353 0.12156863\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.02745098\n",
            " 0.99607843 0.99607843 0.83921569 0.10980392 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.54117647 0.99607843 0.99607843\n",
            " 0.45490196 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.0745098  0.69411765\n",
            " 0.35294118 0.         0.         0.         0.         0.\n",
            " 0.09803922 0.94117647 0.99607843 0.99607843 0.13333333 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.64313725 0.99607843 0.84313725 0.24705882\n",
            " 0.14117647 0.         0.2        0.34901961 0.80784314 0.99607843\n",
            " 0.99607843 0.54509804 0.03137255 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.22352941 0.77254902 0.99607843 0.99607843 0.87058824 0.70588235\n",
            " 0.94509804 0.99607843 0.99607843 0.99215686 0.83529412 0.04313725\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.54901961\n",
            " 0.41176471 0.99607843 0.99607843 0.99607843 0.99607843 0.99607843\n",
            " 0.99607843 0.9254902  0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.02745098 0.45882353\n",
            " 0.45882353 0.64705882 0.99607843 0.99607843 0.9372549  0.19607843\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5MyEbcGosIBG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a003a002-5666-45f0-fabe-6dc2a7f82a30"
      },
      "cell_type": "code",
      "source": [
        "# 1.4. Process the targets: \n",
        "# what can the sigmoid function output versus what would fit best our problem.\n",
        "# The number of output nodes is going to be 10 as we have 10 possible classes\n",
        "no_outputs = 10\n",
        "\n",
        "# Instead of using a single number [0, 9], we'd like a target vector (one-hot encoded).\n",
        "# Basically, we'd like to have:\n",
        "#      1 - for the index of the correct class\n",
        "#      0 - for all the other indexes of the incorrect classes\n",
        "# Example: For label 3, we'd like: [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
        "# HINT: np.zeros\n",
        "\n",
        "targets = np.zeros(no_outputs)\n",
        "targets[int(all_values[0])] = 1\n",
        "print(all_values[0], \"=>\", targets)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 => [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oTacxDQ0sIBK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Generalizarea procesării datelor\n",
        "\n",
        "Putem acum să generalizăm acest pas, făcând o funcţie care ne ia un path către fişierul de intrare şi ne salvează o matrice de dimensiunea (N, 28*28), aplicând procesările de mai sus."
      ]
    },
    {
      "metadata": {
        "id": "5ca2xt4_sIBL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "bfa9fb68-ad13-479f-ff0f-2e794b4d25bd"
      },
      "cell_type": "code",
      "source": [
        "def getData(filePath, numData=-1):\n",
        "    # Open file\n",
        "    data_file = open(filePath, 'r')\n",
        "    # Read lines\n",
        "    data = data_file.readlines()\n",
        "    # Close file\n",
        "    data_file.close()\n",
        "\n",
        "    # numData is the number of samples in the dataset\n",
        "    numData = len(data) if numData == -1 else numData\n",
        "    \n",
        "    # Zeroise the input vector: numData x 784\n",
        "    x = np.zeros((numData, 784))[0 : numData]\n",
        "    # Zeroise the target vector: numData x 10\n",
        "    t = np.zeros((numData, 10))[0 : numData]\n",
        "\n",
        "    # For each sample in the dataset\n",
        "    for i in range(numData):\n",
        "        # Split by ','\n",
        "        item = data[i].split(',')\n",
        "        # Scale the inputs (item[1:]) by 255. They need to fit in the [0, 1] range\n",
        "        item_x = (np.asfarray(item[1:]) / 255.0)\n",
        "        x[i] = item_x\n",
        "        item_t = int(item[0])\n",
        "        t[i, item_t] = 1\n",
        "    return x, t\n",
        "\n",
        "# Get the train data\n",
        "train_x, train_t = getData(\"mnist_train.csv\")\n",
        "\n",
        "# Same for the test data\n",
        "val_x, val_t = getData(\"mnist_test.csv\")\n",
        "\n",
        "# Get an idea about how things look like in the dataset\n",
        "print(train_x.shape, train_t.shape)\n",
        "print(val_x.shape, val_t.shape)\n",
        "\n",
        "# We'll just plot somthing for the fun of it\n",
        "randomIndex = np.random.randint(0, len(train_x))\n",
        "image_array = train_x[randomIndex].reshape((28, 28))\n",
        "plot.imshow(image_array, cmap='Greys', interpolation=None)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784) (60000, 10)\n",
            "(10000, 784) (10000, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f09d88d1cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDLqUwP8D8O9abBw/CmAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f09d89d85f8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "IxuEHRfAsIBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Structura Rețelei\n",
        "\n",
        "Problema recunoașterii cifrelor scrise de mână este o provocare destul de bună cu care să testăm puterea unei rețele neurale. Asta pentru că deja vorbim de a recunoaște ce conține o imagine, cât și despre fuzzyness. Nu este foarte clar cum am putea rezolva problema asta cu un set de reguli dat sau în orice alt fel cu metode computaționale.\n",
        "\n",
        "    2.1. Definim funcțiile de activare & adiacentele:\n",
        "         2.1.1. sigmoida - va trebui să o găsiți voi printre funcționalitățile oferite de scipy.special.\n",
        "         2.1.2. derivata sigmoidei\n",
        "         2.1.3. softmax - aici este necesară o implementare manuală, care lucrează pe batches.\n",
        "         \n",
        "    2.2. Inițializăm rețeaua: stabilim o formă pentru aceasta:\n",
        "         2.2.1. număr de neuroni pe stratul de intrare (no_input), ascuns (no_hidden) și de ieșire (no_output)\n",
        "         2.2.2. matricea de ponderi o determinăm dintr-o distribuție normală cu media aproape zero și std dev = 0.5. \n",
        "                Hint: numpy.random.normal.\n",
        "                În cazul nostru vorbim despre două matrice de ponderi: \n",
        "                 W1 - pentru legăturile dintre stratul de intrare și cel ascuns și \n",
        "                 W2 - pentru legăturile dintre stratul ascuns și cel de ieșire.\n",
        "         2.2.3. Fiecare matrice de ponderi va avea la rândul ei o matrice de gradienţi, care vor fi modificaţi în timpul antrenării. Trebuie să fim foarte antenți ca atunci când ponderile (W1, W2) se modifică, gradienţii se resetează la 0, altfel riscăm să creştem la fiecare pas valorile acestora şi să pierdem controlul asupra reţelei.\n",
        "\n",
        "----------------------------------------------------------------------------------------------------------------------\n",
        "    O sumarizare a funcţionării reţelei este următoarea:\n",
        "    I: (MB, 786)\n",
        "    W1: (786, 100)\n",
        "    W2: (100, 10)\n",
        "\n",
        "    y1 = IxW1 : (MB, 786) x (786, 100) => (MB, 100)\n",
        "    a1 = sigmoid(y1) : (MB, 100)\n",
        "\n",
        "    y2 = a1xW2 : (MB, 100) x (100, 10) = (MB, 10)\n",
        "    a2 = softmax(y2) : (MB, 10)\n",
        "\n",
        "    E = NLL(a2, T) : (MB, 1) (This is called negative log likelihood or (multi class) cross entropy loss)\n",
        "----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    2.3. Forward:\n",
        "         2.3.1. y1 = I * W1\n",
        "         2.3.2. a1 = sigmoid(y1)\n",
        "         2.3.3. y2 = a1 * W2\n",
        "         2.3.4. a2 = softmax(y2)\n",
        "         2.3.5. E = NLL(a2, T)\n",
        "\n",
        "    2.4. Backward:\n",
        "         2.4.1. dE/da2 = can be skipped\n",
        "         2.4.2. dE/dy2 = dE/da2 * da2/dy2\n",
        "                       = NLL'(a2, T) * softmax'(y2) (here we skip demonstration for this)\n",
        "                       = (a2 - T) : (MB, 10)\n",
        "         2.4.3. dE/dW2 = dE/dy2 * dy2/dW2 : (MB, 10) x (MB, 100) => (100, 10)\n",
        "                       = (a2 - T) * a1\n",
        "         2.4.4. dE/da1 = dE/dy2 * dy2/da1 : (MB, 10) x (100, 10) => (MB, 100)\n",
        "                       = (a2 - T) * W2\n",
        "         2.4.5. dE/dy1 = dE/da1 * da1/dy1 : (MB, 100) x (MB, 100) => (MB, 100)\n",
        "                       = (a2 - T) * W2 * sigmoid'(y1)\n",
        "                       = (a2 - T) * W2 * y1 * (1 - y1)\n",
        "         2.4.6. dE/dW1 = dE/dy1 * dy1/dW1 : (MB, 100) x (786, 100) => (786, 100) \n",
        "                       = (a2 - T) * W2 * y1 * (1 - y1) * I\n",
        "\n",
        "    2.5. Testăm rețeaua. Treaba asta se reduce la o propagare înainte a input-urilor folosind ultimul checkpoint sau cel mai bun checkpoint.     "
      ]
    },
    {
      "metadata": {
        "id": "q6hUSU8OsIBP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 2.1. Activation functions\n",
        "\n",
        "def sigmoid(x):\n",
        "    # 2.1.1. Implement the sigmoid function\n",
        "    # Hint scipy.special function\n",
        "    return scipy.special.expit(x)\n",
        "\n",
        "def grad_sigmoid(y):\n",
        "    # 2.1.2. Implement the derivative of the sigmoid function.\n",
        "    # We have multiple ways to compute this, some more efficient than others.\n",
        "    # A recomandation is to clip this value to some fixed interval, [-50, 50] for example, \n",
        "    # so our network doesn't diverge.\n",
        "    # Hint: https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x\n",
        "    grad = y * (1 - y)\n",
        "    return np.clip(grad, -50, 50)\n",
        "\n",
        "def softmax(x):\n",
        "    # 2.1.3. Implement softmax.\n",
        "    # This needs to be batched, so careful with axes. \n",
        "    # Basically, we'll get a matrix of shape: (MB, 10), and we'd like to apply the operator to each line \n",
        "    # independently (not the entire matrix). We offer a few fixes for numerical stability.\n",
        "    # Apply softmax on this new input.\n",
        "    x_new = x - np.max(x, axis=1, keepdims=True)\n",
        "    x_new = np.clip(x_new, -200, np.inf)\n",
        "    exp_inputs = np.exp(x_new)\n",
        "    y = exp_inputs / np.sum(exp_inputs, axis=1, keepdims=True)\n",
        "    return y\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, no_input, no_hidden, no_output):\n",
        "        \n",
        "        # 2.2. Initialize the weights\n",
        "        \n",
        "        # 2.2.1. Set number of nodes in each layer\n",
        "        self.no_input = no_input\n",
        "        self.no_hidden = no_hidden\n",
        "        self.no_output = no_output\n",
        "        \n",
        "        # 2.2.2. Matrices of link weights (b/w input and hidden layer and b/w hidden and output layer)\n",
        "        # Hint: np.random.randn\n",
        "        self.W1 = np.random.randn(self.no_input, self.no_hidden)\n",
        "        self.W2 = np.random.randn(self.no_hidden, self.no_output)\n",
        "\n",
        "        # 2.2.3. Matrices for gradients of each weight.\n",
        "        self.dW1 = np.zeros((self.no_input, self.no_hidden))\n",
        "        self.dW2 = np.zeros((self.no_hidden, self.no_output))\n",
        "\n",
        "    def forward_pass(self, inputs):\n",
        "        # 2.3. The forward pass: from the input layer to the outputs.\n",
        "\n",
        "        # 2.3.1. Compute the inputs of the hidden layer: X_hidden = W1 * I\n",
        "        # HINT: np.dot\n",
        "        #       (1, 784) x (784, 100) = (1, 100)\n",
        "        y1 = np.dot(inputs, self.W1)\n",
        "        \n",
        "        # 2.3.2. Non-linearity (sigmoid) for the intermediate outputs\n",
        "        a1 = sigmoid(y1)\n",
        "\n",
        "        # 2.3.3. Compute the inputs in the hidden layer\n",
        "        # HINT: a1 * W2\n",
        "        y2 = np.dot(a1, self.W2)\n",
        "        \n",
        "        # 2.3.4. Compute the outputs (probabilities)\n",
        "        a2 = softmax(y2)\n",
        "\n",
        "        return inputs, y1, a1, y2, a2\n",
        "\n",
        "    def loss(self, outputs, targets):\n",
        "        T = targets\n",
        "        \n",
        "        # 2.3.5. Compute the loss (1 number) using cross entropy\n",
        "        # Hint: https://stats.stackexchange.com/questions/260505/machine-learning-should-i-use-a-categorical-cross-entropy-or-binary-cross-entro :)\n",
        "        numItems = T.shape[0]\n",
        "        E = T * np.log(outputs[-1] + 1e-7)\n",
        "        E = -np.mean(np.sum(E, axis=1))\n",
        "        return E        \n",
        "\n",
        "    def backward(self, outputs, targets):\n",
        "        # 2.4. Backpropagation\n",
        "        \n",
        "        I, y1, a1, y2, a2 = outputs\n",
        "        T = targets\n",
        "        E = self.loss(outputs, targets)\n",
        "        \n",
        "        # 2.4.1. Skipped\n",
        "\n",
        "        # 2.4.2. Compute dE/dy2, which is the derivative of the last layer\n",
        "        # dE/dy2 = dE/da2 * da2/dy2 = deriv(NLL) * deriv(softmax) = (T - O)\n",
        "        dEdy2 = (a2 - T)\n",
        "\n",
        "        # 2.4.3. Compute dE/dW2 and accumulate gradients for this layer\n",
        "        # dE/dW2 = dE/dy2 * dy2/dW2 = (T - a2) * a1\n",
        "        # (MB, 10) x (MB, 100) => (10, MB) x (MB, 100) => (10, 100) => (100, 10)\n",
        "        # dEdW2 = (T - a2)\n",
        "        dEdW2 = np.dot(dEdy2.T, a1).T\n",
        "        self.dW2 += dEdW2 / len(targets)\n",
        "\n",
        "        # 2.4.4. Compute dE/da1, which is the derivative of the first layer\n",
        "        # dE/da1 = dE/dy2 * dy2/da1\n",
        "        dEda1 = np.dot(dEdy2, self.W2.T)\n",
        "\n",
        "        # 2.4.5. Compute dE/dy1 = dE/da1 * da1/dy1\n",
        "        # da1/dy1 = sigmoid'(y1) = y1 * (1 - y1)\n",
        "        dEdy1 = dEda1 * grad_sigmoid(a1)\n",
        "\n",
        "        # 2.4.6. Compute dE/dw1 = dE/dy1 * dy1/dw1\n",
        "        # dy1/dw1 = I\n",
        "        # Shapes: (1, 100) x (1, 784) => (100, 1) x (1, 784) => (100, 784) => (784, 100)\n",
        "        dEdw1 = np.dot(dEdy1.T, I).T\n",
        "        self.dW1 += dEdw1 / len(targets)\n",
        "\n",
        "        return E\n",
        "\n",
        "    def optimize(self, lr, debug):\n",
        "        self.W1 -= lr * self.dW1\n",
        "        self.W2 -= lr * self.dW2\n",
        "        self.dW1 *= 0\n",
        "        self.dW2 *= 0\n",
        "\n",
        "    def evaluate(self, x, t):\n",
        "        # 2.5. Compute the performance of the network\n",
        "        correct = 0\n",
        "        loss = 0\n",
        "        \n",
        "        # 2.5.1. For each sample in the test set\n",
        "        for i in range(len(x)):\n",
        "            # inputs\n",
        "            inputs = np.expand_dims(x[i], axis=0)\n",
        "            # targets\n",
        "            targets = np.expand_dims(t[i], axis=0)\n",
        "            # compute the outputs (forward pass)\n",
        "            outputs = self.forward_pass(inputs)\n",
        "            \n",
        "            # compute the loss = last loss scaled with 0.999 + current loss\n",
        "            loss = loss * 0.999 + self.loss(outputs, targets)\n",
        "            \n",
        "            indexOutput = np.argmax(outputs[-1], axis=1)\n",
        "            indexTarget = np.argmax(targets, axis=1)\n",
        "            \n",
        "            # increment the number of correct guesses (output is equal to target)\n",
        "            correct += np.sum((indexOutput == indexTarget))\n",
        "\n",
        "        # 2.5.2. Compute the accuracy of the network: how many are correct from the total of predictions \n",
        "        #        (*100 as we want a percentage)\n",
        "        accuracy = correct / len(x) * 100\n",
        "        \n",
        "        # 2.5.3. Ajust the loss \n",
        "        loss /= len(x)\n",
        "        \n",
        "        return loss, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UsME5X65sIBT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Construim și Antrenăm Rețeaua\n",
        "\n",
        "\n",
        "Câteva considerații de luat în seamă despre rețea pot fi văzute mai jos.\n",
        "\n",
        "**Input** \n",
        "    - 784 neuroni. \n",
        "    De ce? \n",
        "    Atâția pixeli am de fapt în imaginile care vin ca input. \n",
        "    Fiecare neuron este un pixel din imaginea de intrare.\n",
        "\n",
        "**Hidden**\n",
        "    - 100 neuroni. \n",
        "    De ce nu mai mulți decât pe stratul de intrare? \n",
        "    Pentru că vrem ca rețeaua să găsească niște pattern-uri în input care pot fi exprimate \n",
        "    într-o formă mai mică decât input-ul în sine. \n",
        "    Vrem un rezumat al input-ului. \n",
        "    Ca să obținem asta, obligăm rețeaua să o facă prin micșorarea numărului de neuroni de pe stratul ascuns.\n",
        "    - adevărul e că alegerea celor 100 de neuroni nu este una foarte științifică. E mai degrabă ceva empiric și de compromis, cumva bazat și pe faptul că la ieșire avem 10 noduri.\n",
        "    - nu există o rețetă după care să alegem câte noduri avem pe stratul ascuns. În general este bine să cunoaștem în profunzime problema cu care avem de-a face. Pe urmă mai este și experiența. Cu cât avansezi în domeniul acesta, îți va veni mai ușor să estimezi cât mai aproape de ideal acel număr.\n",
        "    \n",
        "**Output layer**\n",
        "    - Ce vrem să primim ca răspuns? \n",
        "    Vrem ca rețeaua să ne zică ce cifră avem în imagine. \n",
        "    Prin urmare este de ajuns să returneze un număr de la 0 la 9. \n",
        "    Cu alte cuvinte, pe ultimul strat vom avea 10 neuroni, \n",
        "    fiecare fiind responsabil cu a ne spune ce probabilitate are o cifră dat fiind input-ul. \n",
        "\n",
        "    Deeeci transformă label-ul în vector de 10 elemente, cu 1 pe poziția corespunzătoare cifrei din imagine.\n",
        "    \n",
        "    \n",
        "    3. Antrenarea are următorii pași simpli:\n",
        "        3.1. Se stabileşte un număr de epoci şi o dimensiune a batch-urilor ce sunt propagate la fiecare pas.\n",
        "        3.2. Se aplică pasul de forward pentru batch-ul respectiv (de la stânga la dreapta în reţea)\n",
        "        3.3. Se calculează eroarea pentru batch-ul respectiv (un singur număr)\n",
        "        3.4. Se aplică pasul de backward pentru batch-ul respectiv plecând de la eroarea calculată (de la dreapta\n",
        "             la stânga) şi se acumulează gradienţii pentru parametrii adaptivi (dW1, dW2)\n",
        "        3.5. Folosind algoritmul SGD, se updatează matricile de parametrii (W1, W2) după regula:\n",
        "             Wi[t+1] = Wi[t] - lr * dWi[t]\n",
        "        3.6. Se restează gradienţii pentru ca la următorul batch să nu se cumuleze: dWi *= 0\n",
        "        3.7. La finalul fiecărei epoci, se face un pas de forward pentru setul de validare, se calculează eroarea şi este afişată pentru a vedea dacă reţeaua învaţă.\n",
        "        3.8. Opţional, se salvează doar reţeaua cu cel mai bun loss (sau acurateţe) pe setul de validare.\n",
        "\n",
        "        O euristică ce poate fi ţinută minte în timpul implementării este următoarea: Dacă o anumită intrare are un shape, spre exemplu W1:(100, 10), atunci şi gradientul său va avea acelaşi shape: dW1:(100, 10).\n",
        "\n",
        "        De asemenea, implementarea foloseşte batch size-ul pe prima dimensiune. I:(MB, 28*28), y1:(MB, 100) etc. În schimb, matricile de weights-uri sunt independente de batch size, deci trebuie făcută o medie la calcularea gradienţilor (altfel efectiv mărim learning rate-ul artificial şi putem diverge antrenarea)\n",
        "\n",
        "        Eroarea, fiind de asemenea un singur număr se calculează ca o medie a batch-ului (grijă la dimensiuni).\n",
        "        a2:(MB, 10), T:(MB, 10), E:(MB, 10) -> (MB, ) -> (1, )"
      ]
    },
    {
      "metadata": {
        "id": "HkW9std9sIBU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_nn(no_input, no_hidden, no_output):\n",
        "    return NeuralNetwork(no_input=no_input, no_hidden=no_hidden, no_output=no_output)\n",
        "\n",
        "def build_and_train_nn(no_input, no_hidden, no_output, lr, num_train_data, epochs = 1, printMsg = False):\n",
        "    return train_nn(build_nn(no_input, no_hidden, no_output), lr, num_train_data, epochs, printMsg)\n",
        "\n",
        "def train_nn(nn, lr, num_train_data, epochs=1, printMsg = False):\n",
        "    assert epochs > 0\n",
        "    \n",
        "    train_x, train_t = getData(\"mnist_train.csv\", num_train_data)\n",
        "    val_x, val_t = getData(\"mnist_test.csv\")\n",
        "    \n",
        "    # 3.1. Epochs, batch size and number of steps\n",
        "    numEpochs = 20\n",
        "    MB = 20\n",
        "    numSteps = len(train_x) // MB + (len(train_x) % MB != 0)\n",
        "\n",
        "    print(\"Training a network using %d train images, batch size %d for %d epoch%s.\" \\\n",
        "          % (num_train_data, MB, epochs, \"s\" if epochs > 1 else \"\"))\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Train: use all the samples in the dataset.\n",
        "        for i in range(numSteps):\n",
        "            # 3.2. Forward\n",
        "            # 3.2.1. Get the batch (inputs and targets): each time get the next MB samples\n",
        "            inputs = train_x[i * MB : min((i + 1) * MB, len(train_x))]\n",
        "            targets = train_t[i * MB : min((i + 1) * MB, len(train_t))]\n",
        "\n",
        "            # 3.2.2. Propagate the inputs through the network\n",
        "            out = nn.forward_pass(inputs)\n",
        "            \n",
        "            # 3.3. & 3.4. & 3.5. Compute the loss\n",
        "            loss = nn.backward(out, targets)\n",
        "            \n",
        "            # Print at each 100 steps\n",
        "            if printMsg and i % 100 == 0:\n",
        "                print(\"Epoch %d/%d. Iteration %d/%d. Loss: %2.2f\" % (epoch + 1, numEpochs, i, numSteps, loss), end=\"\\r\")\n",
        "            \n",
        "            # 3.6. Optimize\n",
        "            nn.optimize(lr=0.01, debug=(i % 100 == 0))\n",
        "        \n",
        "        # 3.7. Validation\n",
        "        loss, acc = nn.evaluate(val_x, val_t)\n",
        "        \n",
        "        if printMsg:\n",
        "            print(\"Epoch %d/%d. Val loss: %2.2f. Val acc: %2.2f\" % (epoch + 1, numEpochs, loss, acc))\n",
        "            \n",
        "    return nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Imd6cc7isIBX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experimentul 1\n",
        "\n",
        "### Antrenăm pe 100 exemple, cu 100 de noduri ascunse și learning rate 0.2\n",
        "\n",
        "O observație pe care o vom face aici este că avem o acuratețe extrem de mică. Dezamăgitoare.\n",
        "\n",
        "De ce? \n",
        "Gândiți-vă la complexitatea task-ului pe care îl are de făcut rețeaua. \n",
        "Iar noi îi arătăm doar 100 de imagini apoi avem pretenția să fie capabilă să clasifice corect alte imagini pe care nu le-a mai văzut până acum.\n",
        "\n",
        "Dar rețeaua asta are de diferențiat între 10 clase diferite (cifrele 0-9). \n",
        "Deci în setul de training noi avem maxim câte 10 exemple cu fiecare cifre. \n",
        "Gândiți-vă cât variază curbele și liniile pentru fiecare nou exemplu de cifră scrisă de mână. \n",
        "Rețeaua abia dacă a apucat să-și schițeze câte ceva despre fiecare cifră în timpul antrenamentului. \n",
        "E normală să greșească așa mult."
      ]
    },
    {
      "metadata": {
        "id": "zVDdAQRpsIBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3f018658-7501-4206-ee0f-162dbc5494a3"
      },
      "cell_type": "code",
      "source": [
        "# Set the number of neurons for the input, hidden and output layer.\n",
        "no_input = 784\n",
        "no_hidden = 100\n",
        "no_output = 10\n",
        "\n",
        "# Learning rate tells how fast we want the network to learn.\n",
        "lr = 0.2\n",
        "\n",
        "# Buid and train\n",
        "nn = build_and_train_nn(no_input, no_hidden, no_output, lr, 100)\n",
        "\n",
        "loss, acc = nn.evaluate(val_x, val_t)\n",
        "print(\"Val loss: %2.2f. Val acc: %2.2f\" % (loss, acc))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a network using 100 train images, batch size 20 for 1 epoch.\n",
            "Val loss: 0.92. Val acc: 5.02\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jKUUJa7asIBc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experimentul 2\n",
        "\n",
        "### Antrenăm pe tot setul de date\n",
        "\n",
        "Haideți să vedem ce se întâmplă dacă folosim tot setul de date (60.000 de exemple) pentru antrenare.\n",
        "\n",
        "O să observați o acuratețe de peste 60% până la 78%.\n",
        "De ce credeți că s-a întâmplat asta? Păi pentru simplul fapt că am adăugat mai multe date.\n",
        "În general asta e soluția care pare cea mai simplă."
      ]
    },
    {
      "metadata": {
        "id": "a95jGSbfsIBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "73835dd5-0138-43fb-854b-ceb928d3e1b8"
      },
      "cell_type": "code",
      "source": [
        "# Buid and train\n",
        "nn = build_and_train_nn(no_input, no_hidden, no_output, lr, 60000)\n",
        "loss, acc = nn.evaluate(val_x, val_t)\n",
        "print(\"Val loss: %2.2f. Val acc: %2.2f\" % (loss, acc))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a network using 60000 train images, batch size 20 for 1 epoch.\n",
            "Val loss: 0.12. Val acc: 64.44\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q3ADStotsIBg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experimentul 4\n",
        "\n",
        "### Ne jucăm puțin cu rata de învățare\n",
        "\n",
        "Haideți să vedem ce se întâmplă când nu avem de unde să mai adăugăm date.\n",
        "Pentru că... până la urmă datele în sine sunt greu de obținut. \n",
        "Un set de date presupune o colectare prealabilă a datelor, pregătirea lor (de exemplu scalare în cazul imaginilor din mai multe surse, vectorizare în cazul cuvintelor, etc.), apoi adnotarea lor (punere de etichete de mână) - un proces destul de dureros de multe ori, etc.\n",
        "\n",
        "Ce mai putem face?\n",
        "Să ne jucăm cu learning rate-ul: \n",
        "- mărim\n",
        "- micșorăm.\n",
        "\n",
        "În felul ăsta vedem pe unde ne situăm din punctul de vedere al dependenței acurateții de learning rate.\n",
        "\n",
        "Și mai util ar fi să facem mai multe antrenări și teste, cu diverse valori pentru learning rate, să plotăm un grafic și să ne tragem de acolo concluziile."
      ]
    },
    {
      "metadata": {
        "id": "XQrWLHqzsIBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "52893413-eefc-491f-a5be-e6d55649e51c"
      },
      "cell_type": "code",
      "source": [
        "nn = build_and_train_nn(no_input, no_hidden, no_output, lr * 2, 60000)\n",
        "loss, acc = nn.evaluate(val_x, val_t)\n",
        "print(\"Val loss: %2.2f. Val acc: %2.2f\" % (loss, acc))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a network using 60000 train images, batch size 20 for 1 epoch.\n",
            "Val loss: 0.12. Val acc: 63.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qhbsV2_tsIBl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5868447f-c8a0-4b39-f640-79296cede3c8"
      },
      "cell_type": "code",
      "source": [
        "nn = build_and_train_nn(no_input, no_hidden, no_output, lr / 4, 60000)\n",
        "loss, acc = nn.evaluate(val_x, val_t)\n",
        "print(\"Val loss: %2.2f. Val acc: %2.2f\" % (loss, acc))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a network using 60000 train images, batch size 20 for 1 epoch.\n",
            "Val loss: 0.12. Val acc: 63.30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O8_BEzgGsIBp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experimentul 5\n",
        "\n",
        "### Rulat antrenare de mai multe ori\n",
        "\n",
        "Să zicem.. de 7 ori. Va trebui să așteptăm ceva dar o să merite :)\n",
        "\n",
        "De ce facem asta?\n",
        "Dacă facem gradient descent de mai multe ori, le dăm ocazia ponderilor să se rafineze mai mult. \n",
        "Aplicând gradient descent de mai multe ori dăm mai multe șanse de atingere a valorilor optime pentru acestea.\n",
        "\n",
        "O chestie pe care o vom observa aici este că dacă explorăm timp de mai multe epoci metoda gradient descent, o să ne permitem o rată de învățare mai mică."
      ]
    },
    {
      "metadata": {
        "id": "-EnToDJnsIBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bc1afc26-ce6e-454e-f7be-e20c36022a6e"
      },
      "cell_type": "code",
      "source": [
        "nn = build_and_train_nn(no_input, no_hidden, no_output, lr, 60000, epochs=7)\n",
        "loss, acc = nn.evaluate(val_x, val_t)\n",
        "print(\"Val loss: %2.2f. Val acc: %2.2f\" % (loss, acc))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a network using 60000 train images, batch size 20 for 7 epochs.\n",
            "Val loss: 0.05. Val acc: 84.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iaGh6eeBsIBt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experimentul 6\n",
        "\n",
        "### Schimbăm Arhitectura Rețelei\n",
        "\n",
        "Ce ar mai fi de schimbat?\n",
        "În principiu nu prea putem umbla la stratul de intrare/ieșire din moment ce numărul de neuroni de pe acestea se mapează pe structura datelor de intrare/ieșire.\n",
        "\n",
        "Ar mai rămâne layer-ul ascuns. Aici se întâmplă de fapt învățarea. \n",
        "Dacă privim din avion... aici ajunge input-ul nostru să devină ce vedem la ieșire. \n",
        "De fapt mă refer la ponderile de pe conexiunile de dinainte și după stratul de ieșire. \n",
        "Dar you get the point :)\n",
        "\n",
        "De ce nu e ok să avem mai puțin de 100 noduri?\n",
        "Becoz **learning capacity**. \n",
        "Pur și simplu nu avem cum să înghesuim atâta informație în atât de puțini neuroni.\n",
        "\n",
        "Putem seta la 200 numărul de hidden nodes.\n",
        "\n",
        "Nu uitați că timpul de rulare este direct proporțional cu acest număr. \n",
        "Deci trebuie să luăm și asta în calcul când decidem numărul de hidden nodes. \n",
        "Oricum, dacă facem un grafic al felului cum evoluează acuratețea odată cu creșterea numărului de neuroni de pe stratul de ieșire o să observăm că la un moment dat îmbunătățirile nu mai sunt semnificative indiferent cât am crește acest număr. \n",
        "Cu alte cuvinte, există o plafonare acolo."
      ]
    },
    {
      "metadata": {
        "id": "7vTPh9OpsIBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "73ee45df-13f9-4a6f-a7eb-096018319e31"
      },
      "cell_type": "code",
      "source": [
        "no_hidden = 200\n",
        "nn2 = build_and_train_nn(no_input, no_hidden, no_output, 0.05, 60000, epochs=20, printMsg=True)\n",
        "loss, acc = nn2.evaluate(val_x, val_t)\n",
        "print(\"Val loss: %2.2f. Val acc: %2.2f\" % (loss, acc))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a network using 60000 train images, batch size 20 for 20 epochs.\n",
            "Epoch 1/20. Val loss: 0.14. Val acc: 68.78\n",
            "Epoch 2/20. Val loss: 0.09. Val acc: 77.02\n",
            "Epoch 3/20. Val loss: 0.08. Val acc: 80.77\n",
            "Epoch 4/20. Val loss: 0.07. Val acc: 82.88\n",
            "Epoch 5/20. Val loss: 0.06. Val acc: 84.05\n",
            "Epoch 6/20. Val loss: 0.06. Val acc: 84.95\n",
            "Epoch 7/20. Val loss: 0.05. Val acc: 85.85\n",
            "Epoch 8/20. Val loss: 0.05. Val acc: 86.53\n",
            "Epoch 9/20. Val loss: 0.05. Val acc: 87.15\n",
            "Epoch 10/20. Val loss: 0.05. Val acc: 87.51\n",
            "Epoch 11/20. Val loss: 0.04. Val acc: 87.86\n",
            "Epoch 12/20. Val loss: 0.04. Val acc: 88.09\n",
            "Epoch 13/20. Val loss: 0.04. Val acc: 88.37\n",
            "Epoch 14/20. Val loss: 0.04. Val acc: 88.66\n",
            "Epoch 15/20. Val loss: 0.04. Val acc: 88.87\n",
            "Epoch 16/20. Val loss: 0.04. Val acc: 89.18\n",
            "Epoch 17/20. Val loss: 0.04. Val acc: 89.41\n",
            "Epoch 18/20. Val loss: 0.04. Val acc: 89.59\n",
            "Epoch 19/20. Val loss: 0.03. Val acc: 89.77\n",
            "Epoch 20/20. Val loss: 0.03. Val acc: 89.84\n",
            "Val loss: 0.03. Val acc: 89.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i7tVaMDasIBx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "\n",
        "### Schimbare non-linearitate din sigmoid in ReLU\n",
        "Hint: https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it\n"
      ]
    },
    {
      "metadata": {
        "id": "0hcoj3EEsIBy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5c4be374-ac32-4cbf-a91e-9dcc6ce426e5"
      },
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return x * (x > 0)\n",
        "\n",
        "def grad_relu(x):\n",
        "    return x > 0\n",
        "\n",
        "class NeuralNetworkReLU(NeuralNetwork):\n",
        "    def forward_pass(self, inputs):\n",
        "        # 2.3. The forward pass: from the input layer to the outputs.\n",
        "\n",
        "        y1 = np.dot(inputs, self.W1)\n",
        "        # Change from sigmoid to relu\n",
        "        a1 = relu(y1)\n",
        "\n",
        "        y2 = np.dot(a1, self.W2)\n",
        "        a2 = softmax(y2)\n",
        "\n",
        "        return inputs, y1, a1, y2, a2      \n",
        "\n",
        "    def backward(self, outputs, targets):\n",
        "        # 2.4. Backpropagation\n",
        "        \n",
        "        I, y1, a1, y2, a2 = outputs\n",
        "        T = targets\n",
        "        E = self.loss(outputs, targets)\n",
        "        \n",
        "        dEdy2 = (a2 - T)\n",
        "        dEdW2 = np.dot(dEdy2.T, a1).T\n",
        "        self.dW2 += dEdW2 / len(targets)\n",
        "        dEda1 = np.dot(dEdy2, self.W2.T)\n",
        "        # Change from sigmoid's gradient to relu\n",
        "        # The gradient should be a mask of inputs that are positive, so we only update the\n",
        "        #  weights to these value.\n",
        "        dEdy1 = dEda1 * grad_relu(y1)\n",
        "        dEdw1 = np.dot(dEdy1.T, I).T\n",
        "        self.dW1 += dEdw1 / len(targets)\n",
        "\n",
        "        return E   \n",
        "\n",
        "nn_relu = NeuralNetworkReLU(784, 100, 10)\n",
        "nn_relu = train_nn(nn_relu, 0.05, 60000, epochs=20, printMsg=True)\n",
        "loss, acc = nn_relu.evaluate(val_x, val_t)\n",
        "print(\"Val loss: %2.2f. Val acc: %2.2f\" % (loss, acc))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a network using 60000 train images, batch size 20 for 20 epochs.\n",
            "Epoch 1/20. Val loss: 0.12. Val acc: 85.93\n",
            "Epoch 2/20. Val loss: 0.09. Val acc: 87.37\n",
            "Epoch 3/20. Val loss: 0.08. Val acc: 88.63\n",
            "Epoch 4/20. Val loss: 0.07. Val acc: 89.00\n",
            "Epoch 5/20. Val loss: 0.06. Val acc: 89.18\n",
            "Epoch 6/20. Val loss: 0.06. Val acc: 89.43\n",
            "Epoch 7/20. Val loss: 0.05. Val acc: 89.62\n",
            "Epoch 8/20. Val loss: 0.05. Val acc: 89.74\n",
            "Epoch 9/20. Val loss: 0.05. Val acc: 89.96\n",
            "Epoch 10/20. Val loss: 0.04. Val acc: 90.12\n",
            "Epoch 11/20. Val loss: 0.04. Val acc: 90.35\n",
            "Epoch 12/20. Val loss: 0.04. Val acc: 90.47\n",
            "Epoch 13/20. Val loss: 0.04. Val acc: 90.72\n",
            "Epoch 14/20. Val loss: 0.03. Val acc: 90.93\n",
            "Epoch 15/20. Val loss: 0.03. Val acc: 91.04\n",
            "Epoch 16/20. Val loss: 0.03. Val acc: 91.19\n",
            "Epoch 17/20. Val loss: 0.03. Val acc: 91.34\n",
            "Epoch 18/20. Val loss: 0.03. Val acc: 91.57\n",
            "Epoch 19/20. Val loss: 0.03. Val acc: 91.67\n",
            "Epoch 20/20. Val loss: 0.03. Val acc: 91.75\n",
            "Val loss: 0.03. Val acc: 91.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8bkBuKiMsIB1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bonus 2\n",
        "\n",
        "### Adăugare un nou strat intermediar. Noua arhitectura: 768 -> 300 -> 100 -> 10"
      ]
    },
    {
      "metadata": {
        "id": "yYDGJQK5sIB2",
        "colab_type": "code",
        "colab": {},
        "outputId": "a1d66913-1335-4475-eae9-60de8602eae8"
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork2Hidden(NeuralNetwork):\n",
        "    def __init__(self, no_input, no_hidden1, no_hidden2, no_output):\n",
        "        # Define 3 parameter matrices and 3 gradient matrices (W1, W2, W3, dW1, dW2, dW3)\n",
        "        self.W1 = np.random.randn(no_input, no_hidden1)\n",
        "        self.W2 = np.random.randn(no_hidden1, no_hidden2)\n",
        "        self.W3 = np.random.randn(no_hidden2, no_output)\n",
        "\n",
        "        # 2.2.3. Matrices for gradients of each weight.\n",
        "        self.dW1 = np.zeros((no_input, no_hidden1))\n",
        "        self.dW2 = np.zeros((no_hidden1, no_hidden2))\n",
        "        self.dW3 = np.zeros((no_hidden2, no_output))\n",
        "    \n",
        "    def forward_pass(self, x):\n",
        "        # Compute a1 = relu(IxW1)\n",
        "        y1 = np.dot(x, self.W1)\n",
        "        a1 = sigmoid(y1)\n",
        "\n",
        "        # Compute a2 = sigmoid(a1xW2)\n",
        "        y2 = np.dot(a1, self.W2)\n",
        "        a2 = relu(y2)\n",
        "        \n",
        "        # Compute a3 = softmax(a2xW3)\n",
        "        y3 = np.dot(a2, self.W3)\n",
        "        a3 = softmax(y3)\n",
        "\n",
        "        return x, y1, a1, y2, a2, y3, a3\n",
        "    \n",
        "    def backward(self, outputs, targets):      \n",
        "        I, y1, a1, y2, a2, y3, a3 = outputs\n",
        "        T = targets\n",
        "        E = self.loss(outputs, targets)\n",
        "        \n",
        "        # This should be similar as before (1 hidden layer), but we need new formulas because\n",
        "        #  the network is deeper. \n",
        "        # Compute dE/dy3, dEdW3 (skip dE/da3 as before, because using softmax + NLL)\n",
        "        dEdy3 = (a3 - T)\n",
        "        dEdW3 = np.dot(dEdy3.T, a2).T\n",
        "        # Accumulate (batched) gradients in dW3\n",
        "        self.dW3 += dEdW3 / len(targets)\n",
        "\n",
        "        # Compute dE/da2, dE/dy2, dE/dw2\n",
        "        dEda2 = np.dot(dEdy3, self.W3.T)\n",
        "        dEdy2 = dEda2 * grad_relu(y2)\n",
        "        dEdw2 = np.dot(dEdy2.T, a1).T\n",
        "        # Accumulate (batched) gradients in dW2\n",
        "        self.dW2 += dEdw2 / len(targets)\n",
        "        \n",
        "        # Compute dE/da1, dE/dw1, dEdw1\n",
        "        dEda1 = np.dot(dEdy2, self.W2.T)\n",
        "        dEdy1 = dEda1 * grad_sigmoid(a1)\n",
        "        dEdw1 = np.dot(dEdy1.T, I).T\n",
        "         # Accumulate (batched) gradients in dW1\n",
        "        self.dW1 += dEdw1 / len(targets)\n",
        "\n",
        "        return E   \n",
        "\n",
        "    def optimize(self, lr, debug):\n",
        "        # Update all 3 matrices now\n",
        "        self.W1 -= lr * self.dW1\n",
        "        self.W2 -= lr * self.dW2\n",
        "        self.W3 -= lr * self.dW3\n",
        "        self.dW1 *= 0\n",
        "        self.dW2 *= 0\n",
        "        self.dW3 *= 0\n",
        "    \n",
        "nn_2hidden = NeuralNetwork2Hidden(784, 300, 100, 10)\n",
        "nn_2hidden = train_nn(nn_2hidden, 0.05, 60000, epochs=20, printMsg=True)\n",
        "loss, acc = nn_2hidden.evaluate(val_x, val_t)\n",
        "print(\"Val loss: %2.2f. Val acc: %2.2f\" % (loss, acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training a network using 60000 train images, batch size 20 for 20 epochs.\n",
            "Epoch 1/20. Val loss: 0.12. Val acc: 79.274\n",
            "Epoch 2/20. Val loss: 0.07. Val acc: 79.758\n",
            "Epoch 3/20. Val loss: 0.06. Val acc: 81.464\n",
            "Epoch 4/20. Val loss: 0.05. Val acc: 82.903\n",
            "Epoch 5/20. Val loss: 0.05. Val acc: 84.072\n",
            "Epoch 6/20. Val loss: 0.04. Val acc: 84.930\n",
            "Epoch 7/20. Val loss: 0.04. Val acc: 85.589\n",
            "Epoch 8/20. Val loss: 0.04. Val acc: 86.328\n",
            "Epoch 9/20. Val loss: 0.04. Val acc: 86.817\n",
            "Epoch 10/20. Val loss: 0.04. Val acc: 87.367\n",
            "Epoch 11/20. Val loss: 0.04. Val acc: 87.717\n",
            "Epoch 12/20. Val loss: 0.03. Val acc: 88.086\n",
            "Epoch 13/20. Val loss: 0.03. Val acc: 88.476\n",
            "Epoch 14/20. Val loss: 0.03. Val acc: 88.736\n",
            "Epoch 15/20. Val loss: 0.03. Val acc: 88.946\n",
            "Epoch 16/20. Val loss: 0.03. Val acc: 89.066\n",
            "Epoch 17/20. Val loss: 0.03. Val acc: 89.286\n",
            "Epoch 18/20. Val loss: 0.03. Val acc: 89.565\n",
            "Epoch 19/20. Val loss: 0.03. Val acc: 89.645\n",
            "Epoch 20/20. Val loss: 0.03. Val acc: 89.794\n",
            "Val loss: 0.03. Val acc: 89.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zfuCqFNfsmWV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bonus 3\n",
        "\n",
        "Testăm pe exemple create de noi în paint :).\n",
        "Aveţi grijă să fie de dimensiunea (28, 28) !"
      ]
    },
    {
      "metadata": {
        "id": "gjZiSb9osooX",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "1878caa5-1afd-4a8b-f4f1-9b6cf8345aee"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from scipy import misc\n",
        "\n",
        "!rm my_digit*\n",
        "uploaded = files.upload()\n",
        "\n",
        "image = np.array(misc.imread(\"my_digit.png\"))[..., 0]\n",
        "image = np.float32(image) / 255\n",
        "plot.imshow(image, cmap='Greys', interpolation=None)\n",
        "\n",
        "image = image.reshape((1, 28 * 28))\n",
        "result = nn_relu.forward_pass(image)[-1]\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "print(result)\n",
        "print(np.argmax(result))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-37030b3e-cc0d-4d27-8208-65b90eed2e38\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-37030b3e-cc0d-4d27-8208-65b90eed2e38\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving my_digit.png to my_digit.png\n",
            "[[0.    0.997 0.001 0.001 0.    0.    0.    0.    0.    0.   ]]\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngKFLBRi6VIChSwX8H9Q0CMwBdLcaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f09d865bba8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "VoyS9SmvsrHN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}